\chapter{Исследование существующих функций активации}

\section {Функция активации ReLU}
	Функция активации ReLU (англ. rectified linear unit) определяется следующим образом

	\begin{equation}
		f(x) = \max(0, x),
	\end{equation}
	
	Когда \( x \) меньше 0, значение функции равно 0; когда \( x \) больше или равно 0, значение функции равно самому \( x \). Значительным преимуществом использования функции ReLU является ускорение обучения. Функции сигмоид и тангенс включают экспоненциальную операцию, требующую деление при вычислении производных, тогда как производная ReLU является постоянной. Кроме того, в функциях сигмоида и тангенс, если значение \( x \) слишком велико или слишком мало, градиент функции мал, что может привести к медленной сходимости. Однако, когда \( x \) меньше 0, производная ReLU равна 0, и когда \( x \) больше 0, производная равна 1; таким образом, она может достигать идеального эффекта сходимости \cite{survey}.
	
	\imgScale{0.60}{func_relu}{График функции ReLU}
		
	
	
\section {Функция активации Leaky ReLU}
	Функция активации Leaky ReLU (англ. leaky rectified linear unit) определяется следующим образом

	\begin{equation}
		f(x) = \begin{cases} 
			x & \text{если } x \geq 0 \\
			0.01x & \text{если } x < 0,
		\end{cases}
	\end{equation}
	
	Leaky ReLU — это модификация функции ReLU, которая позволяет избежать проблемы "умирающих нейронов", поскольку она предусматривает небольшой наклон для отрицательных значений \(x\), что позволяет передавать градиент даже когда \(x < 0\) \cite{recursive}.
	
	\imgScale{0.60}{func_leaky_relu}{График функции Leaky ReLU}
	
	
	
\section {Функция активации PReLU}
	Функция активации PReLU (англ. parametric rectified linear unit) определяется следующим образом
	\begin{equation}
		f(x) = \begin{cases} 
			x & \text{если } x \geq 0 \\
			\alpha x & \text{если } x < 0,
		\end{cases}
	\end{equation}
	
	где \(\alpha\) — параметр, который обучается вместе с другими параметрами сети.\newline
	Это позволяет сети динамически адаптировать коэффициент наклона для отрицательных значений в зависимости от характеристик данных. PReLU позволяет отрицательным входам генерировать небольшой отрицательный выход, что может помочь избежать проблемы умирающих нейронов (англ. dying ReLU phenomenon). Это достигается за счет адаптивных параметров $\alpha$, которые обучаются вместе с весами сети. Использование PReLU может ускорить сходимость обучения, не увеличивая значительно вычислительные затраты, так как добавление параметров $\alpha$ не требует значительного увеличения количества операций в гейронной сети. Исходя из того, что количество дополнительных параметров $\alpha$ относительно мало по сравнению с общим числом весов в сети, риск переобучения остается низким.
	Также, параметры $\alpha$ могут быть либо разделяемыми на уровне слоя, либо уникальными для каждого канала, что предоставляет дополнительную гибкость при моделировании различных типов данных и архитектур сетей. ReLU обучается с использованием стандартного метода обратного распространения ошибки, где обновление параметров $\alpha$ выполняется наряду с обновлением весов сети \cite{survey}.
		
	\imgScale{0.60}{func_prelu}{График функции PReLU}

	
	
\section {Функция активации ELU}
	Функция активации ELU (англ. exponential linear unit) определяется следующим образом

	\begin{equation}
		f(x) = 
		\begin{cases} 
			x & \text{если } x \geq 0 \\
			\alpha (e^x - 1) & \text{если } x < 0
		\end{cases}, \quad \text{где } 0 < \alpha.
	\end{equation}
	
	Гиперпараметр \(\alpha\) контролирует значение, к которому ELU насыщается для отрицательных входов. ELU уменьшает проблему исчезающего градиента так же, как и усечённые линейные единицы (ReLU) и утечка ReLU. В отличие от ReLU, ELU имеет отрицательные значения, что приближает среднее активаций к нулю и способствует более быстрому обучению, так как градиент оказывается ближе к натуральному градиенту. ELU насыщается к отрицательному значению, когда аргумент уменьшается. Таким образом, представление данных с помощью ELU является одновременно устойчивым к шуму и не требует сложных вычислений \cite{survey, caltech101}.
	
	\imgScale{0.60}{func_elu}{График функции ELU}
	\newpage
	
	
	
\section {Сигмоид}
	Сигмоид определяется следующим образом

	\begin{equation}
		f(x) = \frac{1}{1 + e^{-x}},
	\end{equation}
	
	Отображает вещественное число в (0, 1), делая её подходящей для задач, где требуется выполнить бинарную классификацию \cite{3dimension}.
	
	\imgScale{0.60}{func_sigmoid}{График сигмоида}
	
\section {Гиперболический тангенс}
	Гиперболический тангенс определяется следующим образом
	\begin{equation}
		f(x) = tanh(x).
	\end{equation}
	
	В отличие от сигмоидной функции, функция гиперболического тангенса отображает действительное число в интервал \((-1, 1)\). Поскольку среднее значение выходных данных функции тангенса равно 0, это позволяет достичь некоторой нормализации. Это облегчает обучение следующего слоя.
	
	\imgScale{0.60}{func_sigmoid}{График гиперболического тангенса}
	
	
\section {Функция активации Swish}
	Swish определяется следующим образом
	\begin{equation}
		f(x) = x \cdot \sigma(\beta x),
	\end{equation}
	
	где $\sigma(z) = (1 + \exp(-z))^{-1}$ — сигмоидная функция, $\beta$ — гиперпараметр.\newline
	Если $\beta = 0$, Swish становится масштабированной линейной функцией $f(x) = x/2$. По мере того как $\beta \to \infty$, сигмоидная составляющая аппроксимируется к функции 0-1, таким образом Swish становится похожим на функцию ReLU. Это предполагает, что Swish можно рассматривать как гладкую функцию, которая нелинейно интерполирует между линейной функцией и функцией ReLU. Степень интерполяции может контролироваться моделью, если $\beta$ установлен как обучаемый параметр \cite{survey, berkeley}.
	
	\imgScale{0.63}{func_swish}{График функции Swish}
	
	
	
\section {Функция активации Mish}
	Mish определяется следующим образом
	\begin{equation}
		f(x) = x \cdot \tanh(\ln(1 + e^x))
	\end{equation}
	
	Благодаря сохранению небольшого количества отрицательной информации, функция активации Mish устраняет предпосылки, необходимые для явления "умирающего ReLU". Будучи неограниченной сверху, данная избегает насыщения, которое обычно приводит к замедлению обучения из-за градиентов, стремящихся к нулю \cite{survey, sparsecnn}.
	
	\imgScale{0.60}{func_mish}{График функции Mish}
	
