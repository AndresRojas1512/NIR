\newcolumntype{Y}{>{\centering\arraybackslash}X}

\chapter{Сравнение функций активации}

В данном разделе определены критерии сравнения функций активации и их оценка по этим критерям.

\begin{itemize}[label=---]
	\item Точность.
		\begin{itemize}
			\item ReLU. Обеспечивает высокую точность при положительных входах, однако сталкивается с проблемой "умирающих нейронов" \cite{mitcnnchapter}.
			\item Leaky ReLU. Модификация ReLU, позволяющая поддерживать градиент при отрицательных значениях, тем самым уменьшая проблему умирающих нейронов.
			\item PReLU. Динамически адаптирует коэффициент наклона при отрицательных значениях за счет обучаемого параметра $\alpha$, улучшая точность.
			\item ELU. Способствует ускоренному обучению за счет улучшенного обработки отрицательных значений и приближения среднего градиента к нулю \cite{fullycnnstruct}.
			\item Сигмоид и гиперболический тангенс. Страдают от насыщения градиентов, что снижает точность, особенно в глубоких сетях.
			\item Swish и Mish. Предоставляют возможность адаптации и улучшенной точности.
		\end{itemize}
	\item Скорость сходимости.
		\begin{itemize}
			\item ReLU и PReLU. Показывают высокую скорость благодаря простоте вычислений \cite{mitcnnchapter}.
			\item ELU. Скорость сходства может быть снижена из-за экспоненциальных вычислений.
			\item Сигмоид и гиперболический тангенс. Имеют замедленную сходимость.
			\item Swish и Mish. Могут обеспечивать улучшенную сходимость в зависимости от контекста применения \cite{survey}.
		\end{itemize}
	\item Градиентный поток.
		\begin{itemize}
			\item ReLU. Имеет ограничения при отрицательных входах.
			\item Leaky ReLU и PReLU. Улучшают градиентный поток благодаря параметру утечки или адаптивному параметру \cite{mitcnnchapter}.
			\item ELU. Обеспечивает сильный градиентный поток при отрицательных значениях.
			\item Swish и Mish. Способствуют более гладкому градиентному потоку.
		\end{itemize}
	\item Вычислительная сложность.
		\begin{itemize}
			\item ReLU и PReLU. Варьируется от низкой до средней.
			\item ELU. Средняя сложность из-за необходимости выполнения экспоненциальных вычислений \cite{sparsecnn}.
			\item Сигмоид и гиперболический тангенс. Высокая сложность из-за сложных функциональных вычислений.
			\item Swish и Mish. Имеют среднюю сложность, но предлагают улучшенные характеристики.
		\end{itemize}
	\item Регуляризация.
		\begin{itemize}
			\item ReLU и PReLU. предлагают минимальные возможности регуляризации, кроме вариантов, таких как PReLU и Leaky ReLU, которые предлагают некоторую гибкость.
			\item ELU. Обеспечивает эффективную регуляризацию за счет учета отрицательных значений \cite{fullycnnstruct}.
			\item Сигмоид и гиперболический тангенс. Увеличенный риск переобучения из-за насыщения.
			\item Swish и Mish. Улучшенные возможности регуляризации.
		\end{itemize}
\end{itemize}


\begin{table}[ht]
	\centering
	\caption{Сравнение функций активации}
	\label{table:activation-functions}
	\renewcommand{\arraystretch}{1.5}
	\begin{tabularx}{\textwidth}{|Y|Y|Y|Y|Y|Y|}
		\hline
		\textbf{Функция активации} & \textbf{Точность} & \textbf{Скорость сходимости} & \textbf{Град. поток} & \textbf{Вычисл. сложность} & \textbf{Регуля-\newlineризация} \\ \hline
		ReLU & Высокая & Высокая & Ограничен & Низкая & Низкая \\ \hline
		Leaky ReLU & Высокая & Средняя & Улучше-\newlineнный & Низкая & Низкая \\ \hline
		PReLU & Высокая & Средняя & Регули-\newlineруемый & Средняя & Средняя \\ \hline
		ELU & Высокая & Высокая & Высокий & Средняя & Улучше-\newlineнная \\ \hline
		Сигмоид & Низкая & Медленная & Ограничен & Высокая & Высокая \\ \hline
		Tanh & Низкая & Медленная & Ограничен & Высокая & Высокая \\ \hline
		Swish & Высокая & Высокая & Гладкий & Средняя & Улучше-\newlineнная \\ \hline
		Mish & Высокая & Высокая & Гладкий & Средняя & Улучше-\newlineнная \\ \hline
	\end{tabularx}
\end{table}
