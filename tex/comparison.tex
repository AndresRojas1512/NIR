\newcolumntype{Y}{>{\centering\arraybackslash}X}

\chapter{Сравнение функций активации}

Для сравнения рассмотренных функций активации была протестирована модель Squeeze Net на эталонном наборе данных CIFAR-10. При тестировании модель Squeeze Net обучалась с нуля. Эсперименты были проведены на процессоре Intel Xeon E5-2640-v4(X2), видеокарте NVIDIA TITAN RTX (24ГБ), в среде языка программироания Python 3.5.6 и с использованием библиотеки Keras 2.2.4. Набор данных CIFAR-10 включает 60 000 изображений в формате RGB размером 32 x 32 пикселей, распределенных по десяти классам, по 6000 изображений в каждом.

Для оценки статистической значимости и стабильности результатов, полученных при применении функции активации Mish по сравнению с другими функциями активации рассматриваются такие показатели как средняя точность ($\mu_{\text{точности}}$), средняя потеря и стандартное отклонение точности.


Результаты тестирования приведены в таблице.

\begin{table}[ht]
	\centering
	\caption{Comparison of Activation Functions}
	\label{tab:activation-functions}
	\begin{tabularx}{0.8\textwidth}{|X|c|c|c|}
		\hline
		Функция активации & $\mu_{\text{точности}}$, \% & $\mu_{\text{потери}}$, \% & $\sigma_{\text{точности}}$ \\
		\hline
		ReLU    & 86.66 & 4.398 & 0.584 \\ \hline
		Leaky ReLU   & 86.85 & 4.112 & 0.4569  \\ \hline
		PReLU    & 87.37 & 4.339 & 0.472  \\ \hline
		ELU    & 86.41 & 4.211 & 0.3371  \\ \hline
		Сигмоид    & 83.91 & 4.831 & 0.5995  \\ \hline
		Tanh    & 82.72 & 5.322 & 0.5826  \\ \hline
		Swish    & 87.32 & 4.22 & 0.414  \\ \hline
		Mish   & 87.48 & 4.13 & 0.3967 \\ \hline
	\end{tabularx}
\end{table}

Фунция активации Mish обеспечивает наивысшую среднуюю точность среди рассматриваемых функций активации ($\mu_{\text{точности}}$), вторую по величине минимальную среднцюю потерю ($\mu_{\text{потери}}$) и третью минимальную стандантную отклонение точности ($\sigma_{\text{точности}}$).
